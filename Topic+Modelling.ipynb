{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Topic Modelling \n",
    "As the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus.\n",
    "\n",
    "Topic Modelling is different from rule-based text mining (regex) . It is an unsupervised approach used for finding and observing the bunch of words (called “topics”) in large clusters of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
    "\n",
    "More formally, we define a **topic** to be a distribution over a fixed vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "New York Times are using topic models to boost their user – article recommendation engines. Various corporations are using topic models for recruitment industries where they aim to extract latent features of job descriptions and map them to right candidates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "A popular topic modeling technique, LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place --> THIS OUR LIKELIHOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that sugar is not good for your lifestyle.\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_features=13, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(doc_complete)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad',\n",
       " 'driving',\n",
       " 'father',\n",
       " 'lot',\n",
       " 'perform',\n",
       " 'practice',\n",
       " 'pressure',\n",
       " 'say',\n",
       " 'school',\n",
       " 'sister',\n",
       " 'spends',\n",
       " 'stress',\n",
       " 'sugar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Min_df: ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "Max_df: ignore terms that have a document frequency strictly higher than the given threshold.\n",
    "\n",
    "If integer is passed occurences are considered, if float b/w 0.0 and 1.0 then frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad',\n",
       " 'driving',\n",
       " 'father',\n",
       " 'lot',\n",
       " 'perform',\n",
       " 'practice',\n",
       " 'pressure',\n",
       " 'say',\n",
       " 'school',\n",
       " 'sister',\n",
       " 'spends',\n",
       " 'stress',\n",
       " 'sugar']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uday/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/uday/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=3, max_iter=40).fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "say sugar sister driving\n",
      "Topic 1:\n",
      "father sister driving school\n",
      "Topic 2:\n",
      "sugar bad pressure stress\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "#display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim and By hand cleaning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/uday/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sugar bad consume sister like sugar father\n",
      "********\n",
      "Sugar is bad to consume. My sister likes to have sugar, but not my father.\n",
      "\n",
      "\n",
      "father spends lot time driving sister around dance practice\n",
      "********\n",
      "My father spends a lot of time driving my sister around to dance practice.\n",
      "\n",
      "\n",
      "doctor suggest driving may cause increased stress blood pressure\n",
      "********\n",
      "Doctors suggest that driving may cause increased stress and blood pressure.\n",
      "\n",
      "\n",
      "sometimes feel pressure perform well school father never seems drive sister better\n",
      "********\n",
      "Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\n",
      "\n",
      "\n",
      "health expert say sugar good lifestyle\n",
      "********\n",
      "Health experts say that sugar is not good for your lifestyle.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    #normalized = set(normalized)\n",
    "    print(normalized)\n",
    "    print(\"********\")\n",
    "    print(doc)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = [set (i) for i in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bad', 'consume', 'father', 'like', 'sister', 'sugar'},\n",
       " {'around',\n",
       "  'dance',\n",
       "  'driving',\n",
       "  'father',\n",
       "  'lot',\n",
       "  'practice',\n",
       "  'sister',\n",
       "  'spends',\n",
       "  'time'},\n",
       " {'blood',\n",
       "  'cause',\n",
       "  'doctor',\n",
       "  'driving',\n",
       "  'increased',\n",
       "  'may',\n",
       "  'pressure',\n",
       "  'stress',\n",
       "  'suggest'},\n",
       " {'better',\n",
       "  'drive',\n",
       "  'father',\n",
       "  'feel',\n",
       "  'never',\n",
       "  'perform',\n",
       "  'pressure',\n",
       "  'school',\n",
       "  'seems',\n",
       "  'sister',\n",
       "  'sometimes',\n",
       "  'well'},\n",
       " {'expert', 'good', 'health', 'lifestyle', 'say', 'sugar'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/19/8ecba86351de0eacb9baf1cc49ba86315cd91bc672acd74d6e4e709eb482/gensim-3.6.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.0MB 929kB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /Users/uday/anaconda3/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /Users/uday/anaconda3/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /Users/uday/anaconda3/lib/python3.6/site-packages (from gensim) (1.15.0)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /Users/uday/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: requests in /Users/uday/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.19.1)\n",
      "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/fe/76fc5a00b0ef8cd7958f2b71e7c442f01ff3883c6f3720a64ddef6b6680b/boto3-1.9.21-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 174kB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/uday/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.24,>=1.21.1 in /Users/uday/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/uday/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.8,>=2.5 in /Users/uday/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2.7)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 3.6MB/s \n",
      "\u001b[?25hCollecting botocore<1.13.0,>=1.12.21 (from boto3->smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/1a/802af8dba189138768281b0278cec9bfed0025b5cc64de5cf1d1e5f271b3/botocore-1.12.21-py2.py3-none-any.whl (4.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.7MB 1.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/uday/anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.21->boto3->smart-open>=1.2.1->gensim) (2.7.3)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /Users/uday/anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.21->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/uday/Library/Caches/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/uday/Library/Caches/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.21 botocore-1.12.21 bz2file-0.98 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim. A module specifically meant for Topic Modelling \n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(2, 1), (4, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(8, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1)],\n",
       " [(2, 1),\n",
       "  (4, 1),\n",
       "  (18, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1)],\n",
       " [(5, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(35 unique tokens: ['bad', 'consume', 'father', 'like', 'sister']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(2, 1), (4, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(8, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1)],\n",
       " [(2, 1),\n",
       "  (4, 1),\n",
       "  (18, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1)],\n",
       " [(5, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.079*\"driving\" + 0.045*\"doctor\" + 0.045*\"may\" + 0.045*\"cause\"'), (1, '0.099*\"sugar\" + 0.056*\"good\" + 0.056*\"health\" + 0.056*\"expert\"'), (2, '0.057*\"father\" + 0.057*\"sister\" + 0.056*\"pressure\" + 0.056*\"never\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Topic 0:\n",
    "pressure say stress\n",
    "Topic 1:\n",
    "sister father sugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
